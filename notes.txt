https://en.wikipedia.org/wiki/Monte_Carlo_tree_search

Leaf parallelization, i.e. parallel execution of many playouts from one leaf of the game tree.
Root parallelization, i.e. building independent game trees in parallel and making the move basing on the root-level branches of all these trees.
Tree parallelization, i.e. parallel building of the same game tree, protecting data from simultaneous writes either with one, global mutex, with more mutexes, or with non-blocking synchronization.

https://arxiv.org/abs/2003.13741
https://deepai.org/publication/parallelization-of-monte-carlo-tree-search-in-continuous-domains
Fig 8 - Leaf max paralleization has best performance

https://www.deepmind.com/blog/alphago-zero-starting-from-scratch
The neural network should observe the state space, then pick the (legal) location of play


Neural network parameters (theta)
-> Run MCTS using (theta) as selection (maximum action value)
-> Generate from the MCTS a policy (PI_t) at each step t
-> Upon a terminal state, update all (PI_t) with who won (z in {-1, 0, 1})
-> Optimize network to minimize difference in (z) and maximize similarity between (theta) and (pi)


**Neural network architecture(in alphaZero paper)**

Input:
(Not including temporal dimension or an 8x8 constant "who's playing" feature)
- 8x8 binary encoded player tiles
  - 0 if empty or other player
  - 1 if player


- 8x8 binary encoded enemy tiles
  - 0 if empty or other player
  - 1 if player


- 8x8 binary encoded legal moves
  - 0 if illegal
  - 1 if legal


**Bold** dd

Output:
- 8x8 vector action probability distribution
- scalar value prediction (-1, 1) [How likely this position is a winning position]

================================================================================

The neural architecture search thing is too hard. Will never get done.
Instead, maybe build incremently more complicated RL models.

There is a finite observation space for Othello
You have:
-> The board state (3 channels)
-> The temporal domain (s_{t-1})

The question is: what function mappings are ideal?
(On the abstract end, think picture of othello state with many time channels,
with an arbitrary graph connecting input state to output state)

((Dream coder might be a good paper to reference here))

In the dream coder paper, they constructed small functional programs that could
make big functional programs

Maybe one could do the same with NN archs

====


-- The trivial network --

In the case of our simple network we have a fully connected multi-layer-perceptetron,
with sparse reward The output will have an 8x8 probability distribution for the board
(illegal moves masked out) and a scalar value that tells us how good our current state is.

We will self play for n games to completion, saving the game state as we go. Then we will 
sample from these game states, optimizing for LSE of the predicted value and the actual game 
result (1, 0, -1)

This is ~sort~ of like a 1D monte carlo tree search?


================================================================================

Multithreading :(

I don't want to do all the inference in stuff in c++ bc kinda defeats the point

https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html

This should do the trick. We can run inference asynchronously, and work in our MCTS/whatever C parallelism no
problem. Sounds too good to be true.

Need to write some callbacks in python for the c prog.

We want to run many games at once the only thing shared between them is the state of the NN for inference

[] = async

  => [1] (async w/ pytorch)
  -> Game has state
  -> NN consumes state, gives prediction (inference)  
  -> Write this data to a buffer [a] (async w/ c prog)
  -> Run some algo on the prediction (best pred, MCTS, etc.)
  -> Generate the next game state


================================================================================

To C or not to C (python lol)

Going back and forth on this but I think the best way forward is to write the networks in python,
then serialize them to a script loaded into a c++ prog (for forward pass inference). Then we can do all of our async operations in one place rather than having to deal with a bunch of really cursed callbacks.

https://pytorch.org/tutorials/advanced/cpp_export.html

(Training will be in python)

The alternative is to rewrite the whole othello lib in python. I don't like this because it's a lot of work and
we take a massive performance penalty. 

================================================================================

-- Trivial Network revisited  / TD learning --

https://www.ai.rug.nl/~mwiering/GROUP/ARTICLES/paper-othello.pdf

Here's the full scheme:

Optimizer(python):
 -> Load model checkpoint
  => Training loop
  -> Sample a memory from self play
    * Encoded as  

Evaluator(c++):


Self play(c++):
  -> Load model script
  -> Create a pool of games
  => For each game
    -> Reserve buffer for saving
    -> Start game
    => Game loop
      -> Run inference on the model
      -> Save the model state and the game state to the buffer
      -> Choose move accordingly
      -> Switch players
      -> End game when appropriate
     
================================================================================

Need to regroup, the previous^ section is a little misleading.

There are several techniques for reinforcement learning. Some we will look at is:
TD learning (predicting just the scalar value of a state)
Q learning (predicting which state in the statespace itself is good)
AlphaZero (predicting which state in the statespace is good, given a MCTS using value scalar)

The architecture for these techniques don't differ a whole lot. TD learning has one output neuron while Q learning/AlphaZero has many output neurons. The junk in the middle can be subject to NAS.

So not only is NAS an important question, so is the training function.


================================================================================

Need to come to a conclusion about what the actual research topic is rn.
My earlier thoughts that NAS would be too hard are wrong. 

My current idea: build AlphaZero for othello, run NAS on the architecture
this simplifies:
  -> only need to build and maintain one algorithm
  -> the ability to produce novel research
    -> reimplmenting a bunch of algos that have been implemented a million times

So how will the NAS work?

Well, if AlphaZero is set up like this:

in -> t1 -> t2 -> ... -> tn -> out
where t is an arbitrary tensor
and -> is an arbitrary activation function

naively: run an optimizer to find the best combination(s)
still naive: run a reinforcement learning algo to find the best combination(s)
maybe less naive: run the former^ in conjunction with an evolutionary algorithm
^ this i ~think~ is current SOTA
*put a paper here so i can confirm this, i know i read it somewhere*

So just come up with ideas in this space

The advantage here is even if this totally flops, it is still likely we can produce something
worthy of contribution-- as far as commentary on this particular subfield goes.

================================================================================

Implementing MCTS:

I have a hashed representation of the game state i am using to search the game state tree.
It would be easier to save the game state with the hashed state in the same struct, i'll probably do that.
Then for inference the process is something like:
  - Search the tree for the state
  - If it does not exist, add it.
  - Otherwise, we have the following 
    - U(s,a) = Q(s,a) + c * P(s,a) * sqrt(sum of N(s,b) for all legal moves b)/1 + N(s,a)
    - c controls exploration
    - Compute which legal move a maximizes U, then play it
    - If the next state s' exists: 
      - call the search tree with s'
    - If it does not exist:
      - add it to the tree and initialize P and v from the neural network
      - return -v
      - [Do not rollout, instead propogate v up the call stack]
    - Compute Q[s][a] = (N[s][a] * Q[s][a] + v)/(N[s][a] + 1)
    - N[s][a] += 1
    - Search tree returns -v


================================================================================
Quick diversion here. There is apparently a way to find legal moves with only bit masking and no loops. Let's see if we can figure out how.



That representation also works but is a simple dual of white,black. 
ulong present = white|black; 
ulong empty = ~(white|black); etc. 
Either way you can find moves w/o loops. 

ulong whiteRight1Captures = empty & ((black >> 1) & mask1) & ((white>>2) & mask2); where the masks are left as an exercise. :)

Actually, nevermind lol. Too much of a diversion to justify the time sink.








Game Over: 1
Black win %: 0.39
White win %: 0.59
Tie %: 0.02
Random self-play
Black win %: 0.5
White win %: 0.47
Tie %: 0.03


=====

45 seconds

=====

# 1
10 25, no cuda
1 min 51 sec

# 2
10 25, no cuda
1 min 42 sec

--
# 1
10 25, cuda
1 min 42 sec

--
10 25, cuda, 4 processes
1 mnin 42 sec each

--
100 25, cuda, 4 processes



export PYTORCH_JIT_USE_NNC_NOT_NVFUSER=1

================================================================================

Hooray for learning. From here:

Create a new folder for each NN
/dev/models/ABC123
Put training examples in 
/dev/models/ABC123/examples.json
Only keep most recent(overwrite for now)
Write checkpoint 
/dev/models/ABC123/checkpoint.(i).pth.tar
Train
Write checkpoint
/dev/models/ABC123/checkpoint.(i+1).pth.tar
Arena
Find best i (curr i vs best i)
if new best i, write 
/dev/models/ABC123/checkpoint.best.pth.tar

====


https://github.com/suragnair/alpha-zero-general/discussions/24
solution for determinism in arena is to used a shared MCTS. Someone pointed out that in
AlphaZero, they axed neural networks fighting and just continually updated. I think we will do this
and keep arena for making elo later.

====

elo library on pip has not been ported to python3, use this guy's fork instead
https://github.com/masfaraud/elo


Goonie Sharon

traxing 
