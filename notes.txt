https://en.wikipedia.org/wiki/Monte_Carlo_tree_search

Leaf parallelization, i.e. parallel execution of many playouts from one leaf of the game tree.
Root parallelization, i.e. building independent game trees in parallel and making the move basing on the root-level branches of all these trees.
Tree parallelization, i.e. parallel building of the same game tree, protecting data from simultaneous writes either with one, global mutex, with more mutexes, or with non-blocking synchronization.

https://arxiv.org/abs/2003.13741
https://deepai.org/publication/parallelization-of-monte-carlo-tree-search-in-continuous-domains
Fig 8 - Leaf max paralleization has best performance

https://www.deepmind.com/blog/alphago-zero-starting-from-scratch
The neural network should observe the state space, then pick the (legal) location of play


Neural network parameters (theta)
-> Run MCTS using (theta) as selection (maximum action value)
-> Generate from the MCTS a policy (PI_t) at each step t
-> Upon a terminal state, update all (PI_t) with who won (z in {-1, 0, 1})
-> Optimize network to minimize difference in (z) and maximize similarity between (theta) and (pi)


**Neural network architecture(in alphaZero paper)**

Input:
(Not including temporal dimension or an 8x8 constant "who's playing" feature)
- 8x8 binary encoded player tiles
  - 0 if empty or other player
  - 1 if player


- 8x8 binary encoded enemy tiles
  - 0 if empty or other player
  - 1 if player


- 8x8 binary encoded legal moves
  - 0 if illegal
  - 1 if legal


**Bold** dd

Output:
- 8x8 vector action probability distribution
- scalar value prediction (-1, 1) [How likely this position is a winning position]

================================================================================

The neural architecture search thing is too hard. Will never get done.
Instead, maybe build incremently more complicated RL models.

There is a finite observation space for Othello
You have:
-> The board state (3 channels)
-> The temporal domain (s_{t-1})

The question is: what function mappings are ideal?
(On the abstract end, think picture of othello state with many time channels,
with an arbitrary graph connecting input state to output state)

((Dream coder might be a good paper to reference here))

In the dream coder paper, they constructed small functional programs that could
make big functional programs

Maybe one could do the same with NN archs

====


-- The trivial network --

In the case of our simple network we have a fully connected multi-layer-perceptetron,
with sparse reward The output will have an 8x8 probability distribution for the board
(illegal moves masked out) and a scalar value that tells us how good our current state is.

We will self play for n games to completion, saving the game state as we go. Then we will 
sample from these game states, optimizing for LSE of the predicted value and the actual game 
result (1, 0, -1)

This is ~sort~ of like a 1D monte carlo tree search?


================================================================================

Multithreading :(

I don't want to do all the inference in stuff in c++ bc kinda defeats the point

https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html

This should do the trick. We can run inference asynchronously, and work in our MCTS/whatever C parallelism no
problem. Sounds too good to be true.

Need to write some callbacks in python for the c prog.

We want to run many games at once the only thing shared between them is the state of the NN for inference

[] = async

  => [1] (async w/ pytorch)
  -> Game has state
  -> NN consumes state, gives prediction (inference)  
  -> Write this data to a buffer [a] (async w/ c prog)
  -> Run some algo on the prediction (best pred, MCTS, etc.)
  -> Generate the next game state


================================================================================

To C or not to C (python lol)

Going back and forth on this but I think the best way forward is to write the networks in python,
then serialize them to a script loaded into a c++ prog (for forward pass inference). Then we can do all of our async operations in one place rather than having to deal with a bunch of really cursed callbacks.

https://pytorch.org/tutorials/advanced/cpp_export.html

(Training will be in python)

The alternative is to rewrite the whole othello lib in python. I don't like this because it's a lot of work and
we take a massive performance penalty. 

================================================================================

-- Trivial Network revisited --

Here's the full scheme:

Optimizer(python):
  

Evaluator(c++):


Self play(c++):
  -> Load model script
  -> Create a pool of games
  => For each game
    -> Reserve buffer for saving
    -> Start game
    => Game loop
      -> Run inference on the model
      -> Save the model state and the game state to the buffer
      -> Choose move accordingly
      -> Switch players
      -> End game when appropriate
     













